{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "27l2lFsRIXGV",
        "r3uZcYz3JGtM",
        "0MpymLx1O3aB",
        "yYAop0jkSpkr",
        "4mfdpIpDTy99"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### import, installs and downloading"
      ],
      "metadata": {
        "id": "27l2lFsRIXGV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aZO45YewIK5d",
        "outputId": "08becd1b-f157-4be9-d9e5-7c059fc8080c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 1.9 MB 9.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 182 kB 53.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 162 kB 36.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 162 kB 26.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 158 kB 43.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 157 kB 12.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 157 kB 13.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 157 kB 45.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 157 kB 49.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 157 kB 49.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 157 kB 19.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 157 kB 46.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 156 kB 45.6 MB/s \n",
            "\u001b[?25h  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-0.10.0-py3-none-any.whl (529 kB)\n",
            "\u001b[K     |████████████████████████████████| 529 kB 13.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (1.21.6)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (21.3)\n",
            "Requirement already satisfied: torch>=1.3.1 in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (1.12.1+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->torchmetrics) (3.0.9)\n",
            "Installing collected packages: torchmetrics\n",
            "Successfully installed torchmetrics-0.10.0\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=10uKko-btA83zgMaUcsz9Cf503S0mpmGh\n",
            "To: /content/train_dataset_train.csv\n",
            "100% 206M/206M [00:00<00:00, 210MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1dN6OKDuYg70V7l9qrjL7CCxQN4W8ROyh\n",
            "To: /content/test_dataset_test.csv\n",
            "100% 84.8M/84.8M [00:00<00:00, 201MB/s]\n",
            "DEVICE: cpu\n"
          ]
        }
      ],
      "source": [
        "!pip install wandb -Uq\n",
        "!pip install torchmetrics\n",
        "\n",
        "import torch\n",
        "import torchmetrics\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import Adam\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import wandb\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import recall_score, precision_score, classification_report, f1_score\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from imblearn.over_sampling import ADASYN\n",
        "from imblearn.over_sampling import SMOTE \n",
        "\n",
        "!gdown 10uKko-btA83zgMaUcsz9Cf503S0mpmGh -O /content/train_dataset_train.csv\n",
        "!gdown 1dN6OKDuYg70V7l9qrjL7CCxQN4W8ROyh -O /content/test_dataset_test.csv\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print('DEVICE:', device)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### dataset splitting"
      ],
      "metadata": {
        "id": "r3uZcYz3JGtM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize preprocessors for data\n",
        "scaler = preprocessing.StandardScaler()\n",
        "rus = RandomUnderSampler(sampling_strategy={0:24000,\n",
        "                                            1:24000,\n",
        "                                            3:24000, \n",
        "                                            4:24000,\n",
        "                                            5:24000\n",
        "                                            }, random_state=44)\n",
        "adasyn = ADASYN(sampling_strategy='not majority')\n",
        "\n",
        "df = pd.read_csv('/content/train_dataset_train.csv')\n",
        "\n",
        "df_predict = pd.read_csv('/content/test_dataset_test.csv')\n",
        "\n",
        "# get features and targets\n",
        "X = df.drop([\"Class\", \"id\"], axis = 1)\n",
        "y = df[[\"Class\"]]\n",
        "\n",
        "X_predict = df_predict.drop([\"id\"], axis = 1)\n",
        "id = df_predict[\"id\"]\n",
        "\n",
        "# train <-> test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=44)\n",
        "\n",
        "# scale data\n",
        "X_train = pd.DataFrame(scaler.fit_transform(X_train), columns=['Easting', 'Northing', 'Height','Reflectance'])\n",
        "X_test = pd.DataFrame(scaler.fit_transform(X_test), columns=['Easting', 'Northing', 'Height','Reflectance'])\n",
        "\n",
        "X_predict = pd.DataFrame(scaler.fit_transform(X_predict), columns=['Easting', 'Northing', 'Height','Reflectance'])\n",
        "\n",
        "# resample data\n",
        "print('before resampling:', '\\n', y_train.value_counts())\n",
        "X_train, y_train = rus.fit_resample(X_train, y_train)\n",
        "print('after under sampling:', '\\n', y_train.value_counts())\n",
        "# X_train, y_train = adasyn.fit_resample(X_train, y_train)\n",
        "X_train, y_train = SMOTE({64:8000}).fit_resample(X_train, y_train)\n",
        "print('after over sampling:', '\\n', y_train.value_counts())\n",
        "\n",
        "# to nunmpy\n",
        "X_train = X_train.to_numpy()\n",
        "y_train = y_train.to_numpy().reshape(-1)\n",
        "X_test = X_test.to_numpy()\n",
        "y_test = y_test.to_numpy().reshape(-1)\n",
        "\n",
        "X_predict = X_predict.to_numpy()\n",
        "id = id.to_numpy().reshape(-1)\n",
        "\n",
        "# train <-> valid split\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.1, random_state=44)\n",
        "print('train:', y_train.shape, '; valid:', y_valid.shape, '; test:', y_test.shape)\n",
        "\n",
        "# reset Class id\n",
        "y_train = np.where(y_train==3, 2, y_train)\n",
        "y_train = np.where(y_train==4, 3, y_train)\n",
        "y_train = np.where(y_train==5, 4, y_train)\n",
        "y_train = np.where(y_train==64, 5, y_train)\n",
        "y_valid = np.where(y_valid==3, 2, y_valid)\n",
        "y_valid = np.where(y_valid==4, 3, y_valid)\n",
        "y_valid = np.where(y_valid==5, 4, y_valid)\n",
        "y_valid = np.where(y_valid==64, 5, y_valid)\n",
        "y_test = np.where(y_test==3, 2, y_test)\n",
        "y_test = np.where(y_test==4, 3, y_test)\n",
        "y_test = np.where(y_test==5, 4, y_test)\n",
        "y_test = np.where(y_test==64, 5, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gK-vyVEYI7T1",
        "outputId": "0f25d45a-9066-4002-ea9f-e6d15e3e27e4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "before resampling: \n",
            " Class\n",
            "0        2457843\n",
            "3        1159329\n",
            "4          93106\n",
            "5          50306\n",
            "1          35332\n",
            "64          3968\n",
            "dtype: int64\n",
            "after under sampling: \n",
            " Class\n",
            "0        24000\n",
            "1        24000\n",
            "3        24000\n",
            "4        24000\n",
            "5        24000\n",
            "64        3968\n",
            "dtype: int64\n",
            "after over sampling: \n",
            " Class\n",
            "0        24000\n",
            "1        24000\n",
            "3        24000\n",
            "4        24000\n",
            "5        24000\n",
            "64        8000\n",
            "dtype: int64\n",
            "train: (115200,) ; valid: (12800,) ; test: (422210,)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/imblearn/utils/_validation.py:591: FutureWarning: Pass sampling_strategy={64: 8000} as keyword args. From version 0.9 passing these as positional arguments will result in an error\n",
            "  FutureWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### to tensor"
      ],
      "metadata": {
        "id": "0MpymLx1O3aB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RailsDataset(Dataset):\n",
        "  \n",
        "    def __init__(self, X, y):\n",
        "\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "\n",
        "        self.features = torch.Tensor(X)\n",
        "        self.targets = torch.LongTensor(y)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.features.shape[0]\n",
        "  \n",
        "    def __getitem__(self, index):\n",
        "        return (self.features[index], self.targets[index])"
      ],
      "metadata": {
        "id": "V0AQrj_FO7lY"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = RailsDataset(X_train, y_train)\n",
        "valid_dataset = RailsDataset(X_valid, y_valid)\n",
        "test_dataset = RailsDataset(X_test, y_test)\n",
        "\n",
        "predict_dataset = RailsDataset(X_predict, id)"
      ],
      "metadata": {
        "id": "SrGBctlrQIyi"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### functions to train and test"
      ],
      "metadata": {
        "id": "yYAop0jkSpkr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def trainer(model, train_loader, valid_loader, loss_function, optimizer, scheduler, config):\n",
        "    \"\"\"\n",
        "    (count_of_epoch, batch_size, dataset, model, loss_function, optimizer, lr = 0.001)\n",
        "    trainer итерируется по кол-ву эпох и вызывает функцию train_epoch\n",
        "    count_of_epoch - кол-во эпох\n",
        "    batch_size - размер батча\n",
        "    dataset - данные для обучения\n",
        "    model - модель нейронной сети\n",
        "    loss_function - функция потерь\n",
        "    optimizer - оптимизатор\n",
        "    lr - скорость обучения, по умолчанию 0.001\n",
        "    \"\"\"\n",
        "    min_valid_loss = np.inf\n",
        "\n",
        "    # # in this foulder will save model weights\n",
        "    if not os.path.exists('/content/model_weights'):\n",
        "        os.mkdir('/content/model_weights')\n",
        "\n",
        "    # Tell wandb to watch what the model gets up to: gradients, weights, and more!\n",
        "    wandb.watch(model, loss_function, log=\"all\", log_freq=10)\n",
        "    \n",
        "    for e in range(config.count_of_epoch):\n",
        "        # train\n",
        "        epoch_loss = train_epoch(train_generator=train_loader, \n",
        "                    model=model, \n",
        "                    loss_function=loss_function, \n",
        "                    optimizer=optimizer)\n",
        "\n",
        "        # valid\n",
        "        valid_loss = 0.0\n",
        "        model.eval()\n",
        "        valid_loss = train_epoch(train_generator=valid_loader, \n",
        "                    model=model, \n",
        "                    loss_function=loss_function, \n",
        "                    optimizer=optimizer)\n",
        "        \n",
        "        scheduler.step(epoch_loss)\n",
        "\n",
        "        # log things\n",
        "        trainer_log(epoch_loss, valid_loss, e, optimizer.param_groups[0]['lr'], min_valid_loss)\n",
        "\n",
        "        # saving models\n",
        "        if min_valid_loss > valid_loss:\n",
        "            print(f'Validation Loss Decreased({min_valid_loss:.6f}--->{valid_loss:.6f}) \\t Saving The Model')\n",
        "            min_valid_loss = valid_loss\n",
        "            torch.save(model.state_dict(), f'/content/model_weights/saved_model_{e}.pth')\n",
        "            wandb.log_artifact(f'/content/model_weights/saved_model_{e}.pth', \n",
        "                               name=f'saved_model_{e}', type='model')\n",
        "        print()\n",
        "\n",
        "def train_epoch(train_generator, model, loss_function, optimizer):\n",
        "    \"\"\"\n",
        "    внутри train_epoch итерируемся по батчам внутри батчгенератора\n",
        "    train_generator - батчгенератора\n",
        "    model - модель нейронной сети\n",
        "    loss_function - функция потерь\n",
        "    optimizer - оптимизатор\n",
        "    \"\"\"\n",
        "    epoch_loss = 0\n",
        "    total = 0\n",
        "    for it, (batch_of_x, batch_of_y) in enumerate(train_generator):\n",
        "        batch_loss = train_on_batch(model, batch_of_x, batch_of_y, optimizer, loss_function)\n",
        "            \n",
        "        epoch_loss += batch_loss*len(batch_of_x)\n",
        "        total += len(batch_of_x)\n",
        "    \n",
        "    return epoch_loss/total\n",
        "\n",
        "def train_on_batch(model, x_batch, y_batch, optimizer, loss_function):\n",
        "    \"\"\"\n",
        "    в train_on_batch обучаемся на одном батче\n",
        "    model - модель нейронной сети\n",
        "    x_batch - фичи\n",
        "    y_batch - таргеты(метки классов)\n",
        "    optimizer - оптимизатор\n",
        "    loss_function - функция потерь\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    output = model(x_batch.to(device))\n",
        "\n",
        "    loss = loss_function(output, y_batch.to(device))\n",
        "    # loss.requires_grad = True\n",
        "    loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "    return loss.cpu().item()\n",
        "\n",
        "def tester(model, test_loader):\n",
        "    pred = []\n",
        "    real = [] \n",
        "    model.eval()\n",
        "    for it, (x_batch, y_batch) in enumerate(test_loader):\n",
        "        x_batch = x_batch.to(device)\n",
        "        with torch.no_grad():\n",
        "            output = model(x_batch)\n",
        "\n",
        "        pred.extend(torch.argmax(output, dim=-1).cpu().numpy().tolist())\n",
        "        real.extend(y_batch.cpu().numpy().tolist())\n",
        "\n",
        "    wandb.log({\"test_recall\": recall_score(real, pred, average='macro')})\n",
        "    print('Recall:', recall_score(real, pred, average='macro'))\n",
        "    print('Recall weights:', recall_score(real, pred, average=None))\n",
        "    print(classification_report(real, pred, zero_division = 0))\n",
        "\n",
        "def predicter(model, predict_loader):\n",
        "    pred = []\n",
        "    ids = []\n",
        "    model.eval()\n",
        "    for it, (x_batch, y_batch) in enumerate(predict_loader):\n",
        "        x_batch = x_batch.to(device)\n",
        "        with torch.no_grad():\n",
        "            output = model(x_batch)\n",
        "\n",
        "        pred.extend(torch.argmax(output, dim=-1).cpu().numpy().tolist())\n",
        "        ids.extend(y_batch.cpu().numpy().tolist())\n",
        "    \n",
        "    pred = np.array(pred)\n",
        "    pred = np.where(pred==5, 64, pred)\n",
        "    pred = np.where(pred==4, 5, pred)\n",
        "    pred = np.where(pred==3, 4, pred)\n",
        "    pred = np.where(pred==2, 3, pred)\n",
        "    predict_df = pd.concat([pd.Series(ids, name='id'), pd.Series(pred, name='Class')], axis=1)\n",
        "    predict_df.to_csv('prediction.csv', sep=',', index=False)\n",
        "\n",
        "def trainer_log(train_loss, valid_loss, epoch, lr, min_val_loss):\n",
        "    wandb.log({'train_loss': train_loss, 'valid_loss': valid_loss,\n",
        "               'epoch': epoch, 'learning_rate': lr,\n",
        "               'min_validation_loss': min_val_loss})\n",
        "    print(f'train loss on {str(epoch).zfill(3)} epoch: {train_loss:.6f} with lr: {lr:.10f}')\n",
        "    print(f'valid loss on {str(epoch).zfill(3)} epoch: {valid_loss:.6f}')\n",
        "\n",
        "def make_loader(dataset, batch_size):\n",
        "    loader = torch.utils.data.DataLoader(dataset=dataset,\n",
        "                                         batch_size=batch_size, \n",
        "                                         shuffle=False,\n",
        "                                         pin_memory=True, num_workers=2)\n",
        "    loader = tqdm(loader, )\n",
        "    return loader"
      ],
      "metadata": {
        "id": "rpzFXDtMSusB"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pipeline"
      ],
      "metadata": {
        "id": "4mfdpIpDTy99"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pipeline(hyperparameters, saved_model=None, to_train=True, to_test=True, to_predict=False):\n",
        "\n",
        "    with wandb.init(project=hyperparameters['project'], config=hyperparameters) as run:\n",
        "      config = wandb.config\n",
        "      \n",
        "      # build the model\n",
        "      model = build_model(run, config, saved_model)\n",
        "\n",
        "      # make the data and optimization \n",
        "      train_loader, valid_loader, test_loader, predict_loader, criterion, optimizer, scheduler = make(model, config)\n",
        "\n",
        "      print('config:', '\\n', config, '\\n', model, '\\n', 'running on device:', device, '\\n')\n",
        "\n",
        "      if to_train:\n",
        "        trainer(model, train_loader, valid_loader, criterion, optimizer, scheduler, config)\n",
        "\n",
        "      if to_test:\n",
        "        tester(model, test_loader)\n",
        "\n",
        "      if to_predict:\n",
        "        predicter(model, predict_loader)\n",
        "\n",
        "    return model\n",
        "\n",
        "def build_model(run, config, saved_model=None):\n",
        "    IN, H1, H2, H3, H4, H5, OUT = 4, 256, 128, 64, 32, 16, 6\n",
        "    p = config.dropout\n",
        "\n",
        "    model =  nn.Sequential(\n",
        "    nn.Linear(IN, H1), nn.Dropout(p), nn.BatchNorm1d(H1), nn.ReLU(),        \n",
        "    nn.Linear(H1, H2), nn.Dropout(p), nn.BatchNorm1d(H2), nn.ReLU(), \n",
        "    nn.Linear(H2, H3), nn.Dropout(p), nn.BatchNorm1d(H3), nn.ReLU(),  \n",
        "    nn.Linear(H3, H4), nn.Dropout(p), nn.BatchNorm1d(H4), nn.ReLU(),  \n",
        "    nn.Linear(H4, H5), nn.Dropout(p), nn.BatchNorm1d(H5), nn.ReLU(),  \n",
        "    nn.Linear(H5, OUT), nn.Dropout(p), nn.BatchNorm1d(OUT)) \n",
        "\n",
        "    if saved_model is not None:\n",
        "        artifact = run.use_artifact(f'abletobetable/{config.project}/saved_model_{saved_model[0]}:{saved_model[1]}', type='model')\n",
        "        artifact_dir = artifact.download() + f'/saved_model_{saved_model[0]}.pth'\n",
        "        model.load_state_dict(torch.load(artifact_dir, map_location=torch.device(device)))\n",
        "    else:\n",
        "        def init_weights(m):\n",
        "            if type(m) == nn.Linear:\n",
        "                torch.nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n",
        "        model.apply(init_weights)\n",
        "\n",
        "    model = model.to(device)\n",
        "    return model\n",
        "\n",
        "def Recall(prediction, real, eps=1e-8):\n",
        "    \n",
        "    torch_recall = torchmetrics.Recall(6, 0.5, 'macro').to(device)\n",
        "    pred = torch.argmax(prediction, dim=1)\n",
        "\n",
        "    return torch_recall(pred, real)\n",
        "\n",
        "def make_criterion():\n",
        "    def recall_criterion(pred, real):\n",
        "        return torch.tensor(1) - Recall(torch.exp(pred), real)\n",
        "    return recall_criterion\n",
        "\n",
        "def make(model, config):\n",
        "\n",
        "    # if to train and test\n",
        "    if train_dataset is not None: \n",
        "        train_loader = make_loader(train_dataset, batch_size=config.batch_size)\n",
        "        valid_loader = make_loader(valid_dataset, batch_size=config.batch_size)\n",
        "        test_loader = make_loader(test_dataset, batch_size=config.batch_size)\n",
        "        predict_loader = make_loader(predict_dataset, batch_size=config.batch_size)\n",
        "\n",
        "    # if only to test\n",
        "    else:  \n",
        "        train_loader = None\n",
        "        valid_loader = None\n",
        "        predict_loader = None\n",
        "        test_loader = make_loader(test_dataset, batch_size=config.batch_size)\n",
        "    \n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    # criterion = make_criterion()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=config.lr)\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience = config.patience, \n",
        "                                                           factor = config.step_gamma, min_lr=1e-8)\n",
        "    # scheduler = StepLR(optimizer, config.step_size, config.step_gamma)\n",
        "    \n",
        "    return train_loader, valid_loader, test_loader, predict_loader, criterion, optimizer, scheduler"
      ],
      "metadata": {
        "id": "dBP2O3CIUWqt"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Running"
      ],
      "metadata": {
        "id": "UyM0Ca01ZNnQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.init()"
      ],
      "metadata": {
        "id": "q8GygEjOZguE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = dict(count_of_epoch=10000, batch_size=128, lr=1e-2, \n",
        "              dropout=0.001, critirion='Recall', \n",
        "              optimizer='Adam', scheduler='ReducePlateau', \n",
        "              step_size = 250, step_gamma = 0.1, patience=25,\n",
        "              project='rails', name_of_model='mlp')\n",
        "\n",
        "\n",
        "model = pipeline(config, saved_model=None, to_train=True, to_test=True, to_predict=False)"
      ],
      "metadata": {
        "id": "PgLXApAQZIf2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# predict"
      ],
      "metadata": {
        "id": "Bghpvhszp-dn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config = dict(count_of_epoch=1, batch_size=256, lr=1e-2, \n",
        "              dropout=0.0, critirion='Recall', \n",
        "              optimizer='Adam', scheduler='ReducePlateau', \n",
        "              step_size = 250, step_gamma = 0.1, patience=25,\n",
        "              project='rails', name_of_model='mlp')\n",
        "\n",
        "saved_model = ['1404', 'v0']\n",
        "\n",
        "model = pipeline(config, saved_model=saved_model, to_train=False, to_test=True, to_predict=True)"
      ],
      "metadata": {
        "id": "yd2kIKWMKMhm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}